---
title: "Scraping PDFs in R"
author: "Jared Berry"
date: "January 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Knowing how to source data from non-traditional sources (i.e. somewhere where the data can't be easily downloaded in a neat, tabular way) is a powerful skill for any data scientist or analyst to have in their toolkit. Most commonly, this takes the form of webscraping (I'm looking at you BeautifulSoup and rvest) or pulling data through an API, like the Twitter API. Scraping data from PDFs is likely a slightly less-populated corner of this world, but valuable nonetheless. If your team relies on data that comes from, say, a PDF newsletter, that gets fed into an automated data pipeline, you're *generally* left with two clear options: either the data can be highlighted, copied, and pasted (but will still need to be formatted manually) or hand key the values into another, more tabular format so they can be used in a more automated fashion (which requires even more manual work, and is even more vulnerable to human error). 

Fortunately, as with most things in R (and Python, of course) there is a package for that- in this case I'll be using `pdftools` to scrape tables from a statistical release produced by the [Federal Reserve Board](https://www.federalreserve.gov/releases/g20/current/g20.pdf).I want to focus more on the general workflow, rather than the specifics necessary to scrape *this particular* PDF. Since the primary function in the `pdftools` package pulls the contents of PDF as raw text, regular expressions will be exceptionally useful for making this simple and flexible. With the `pdftools` library, a bit of regular expression know-how, and a *bit* of standardization (if the document you're working with changes every time, no amount of regular expressions will make it feasible to automate), I hope this can be more broadly applicable to other, similar data sources and can save time and frustration.

## Set-up 

As always, I'm operating under the assumption that the reader has R/RStudio installed on their device. In addition to `pdftools`, I'll be using a few other libraries from the `tidyverse`, predominantly `stringr`, which provides a better suite of functions for working with strings and regular expressions than is available in base R. The others, `purrr` is here for use of the `map` function, which provides a cleaner and arguably more [stable](http://r4ds.had.co.nz/iteration.html) alternative to the base R `apply` family.

```{r wd, echo=FALSE}
setwd("C:/Users/Jared/Documents/datasci/projects/pdf_scrape")
```

```{r setlibs, message=FALSE}
library(pdftools)
library(stringr)
library(purrr)
```

## Extracting the table

With the libraries attached, the obvious first step is to do the initial scraping of our PDF document. `pdftools` makes this a trivial exercise, and most of the work in this post will be devoted to a brief overview of regular expressions and string manipulation to get everything formatted nicely. Simply calling the `pdf_text` function on a pdf document scrapes the text from the document and stores in a variable of our choosing.

```{r pdfscrape}
# Set path to the newsletter we want to scrape
pdf_name <- "g20.pdf"

# Read in newsletter text using the pdftools module
g20_text <- pdf_text(pdf_name)
```

So what does this look like? `pdf_text` creates a character vector, where each element is a page of the document, formatted as a single character string. To get a sense of what we're working with, we can examine the first 1,000 characters of the first page.

```{r showtext}
str(g20_text)
substr(g20_text[1],1,1000)
```

Each page contains (at least) a table to scrape. In this example, I'll work with the table on the second page, which looks like this ![](table1.png):

```{r getlastpage}
# Separate out the page which contains the table of interest
table_page <- g20_text[[2]]

# Inspect
substr(table_page,1,1000)
```

At present, this looks more like a garbled mess of text than a workable table. Fortunately, it's a fairly well-structured garbled mess of text, and with a bit of regular expression magic, can be condensed into a tidy bit of tabular data. The first thing to do is convert this single, long string into reasonable pieces. Using `str_split` from the `stringr` package, I can pass in a character upon which to split a string into either the elements of a vector (using the `simplify` argument) or list. So, the string 'a|b|c|d|e|f' could be broken out as, splitting on the '|' character:
```{r demsplit, echo=FALSE}
str_split('a|b|c|d|e|f', '\\|', simplify=TRUE)
```
Here, I'll break at the new line symbol, or "\\r\\n":
```{r split}
# Split full table into individual lines of text based on '\n' line break character
g20_table_raw <- str_split(table_page, "\r\n", simplify=TRUE)
str(g20_table_raw)
print(g20_table_raw)
```

This already resembles a table, and it's only required a few lines of code to get here. For the rest of the formatting and cleaning, I'll use two other functions from the `stringr` package: `str_detect`, and `str_extract`. As with most functions in the `Tidyverse`, the names are fairly self explanatory: `str_detect` will detect the location of a given string in a vector of strings, and return `TRUE` where it is found and `FALSE` otherwise, much like the base R `grepl` function. Unsurprisingly, `str_extract` will do the same, but instead return the chosen string or regular expression.

```{r strinaction}
# A vector of strings
fruit_str <- c("apples", "oranges", "blueberries", "pineapples", "watermelon", "strawberries")

# str_detect
apples_lgl <- str_detect(fruit_str, "apples")
fruit_str[apples_lgl]

# str_extract
str_extract(fruit_str, "apples")
```

These functions detect or extract *exactly* what has been passed in as the `pattern` argument to the function. In order to build in some more power and a lot more flexibility, regular expressions can be exceptionally useful. I intend for this to serve as a fairly gentle introduction to regular expressions - they can be immeasurably powerful, but get exceptionally complicated *very* quickly. I have a copy of [this](https://www.rstudio.com/wp-content/uploads/2016/09/RegExCheatsheet.pdf) cheat sheet posted up in my office, and [Rex Egg](https://www.rexegg.com/regex-quickstart.html) is another great resource. I am by no means a regular expression master, but have developed enough of a command to make my life easier for automating these kinds of tasks. In order to prevent this from becoming a massive regular expression tutorial, I'm going to build my expressions before using them in the `stringr` functions and explain exactly what they're doing, rather than providing a comprehensive overview. I'd encourage the reader to check out the resources for building out there regex knowledge.

Looking at the excerpt of the table above, I'm interested in dropping the header info. I can detect these by wrapping them in parentheses and separating them with the '|' character, which essentially says detect 'Owned' OR 'Footnotes' OR 'Billions' OR adjusted. After extracting the location of these strings, I can drop their indexes from the vector. With only table entries to operate with, I'll be using `str_split` again to split at spaces.  The `"\\s+"` expression splits at all space characters (`"\\s"`) that match at least one time (`+`) - this ensures we split at the blocks of spaces that have different lengths. Before that though, note the formatting of the first 'real' row of table entries. The commas, used to denote thousands, are going to cause us grief when we convert the values to numeric later on. So, before splitting things up, we can use `str_replace_all` to remove those pesky commas.

```{r pullnsplit}
# Assemble a regular expression to identify table entries of interest
row_expr <- paste(c("Owned", "Footnotes", "Billions", "adjusted"), collapse = "|")
row_expr

# Remove rows which do not contain values for input based on names from PDF
g20_table <- g20_table_raw[which(!str_detect(g20_table_raw, row_expr))]

# Remove commmas
g20_table <- map_chr(g20_table, str_replace_all, pattern=",", replacement="")

# Split at any number of spaces
elements <- map(g20_table, function(x) str_split(x, "\\s+", simplify = TRUE))
print(elements[[4]])
```

With these two commands, we're nearly done formatting the values for the table. Looking at the elements list, there are numeric values from the table mixed with empty strings, and row. In order to ensure we strip the unnecessary text, but keep the values, I'll need to build something a bit more flexible. The `entry_expr` expression below will detect the `[0-9]*\\.[0-9]*` pattern. `[0-9]*` will detect any digits (`[0-9]`), any number of times (the `*` modifer matches the digits any number of times). This will capture the whole number portions of the table entries. In order to capture the decimal point, I 'escape' it, as before, since the `.` means something special in regular expressions.I detect this expression vector by vector in the `map` function, and convert each piece to numeric. After binding the elements of the list together, we finally have the values neatly formatted exactly as they appear in the table.

```{r formatnum, warning=FALSE}
# Remove unnecessary text to convert table values from text to numeric
entry_expr <- "[0-9]*\\.[0-9]*"
num_values <- map(elements, function(x) as.numeric(x[str_detect(x, entry_expr)]))

g20_values <- do.call("rbind", num_values)
print(g20_values)
```

Though things have been split up with my explanations, it only took about *10 lines of code* to take the PDF document and convert the tables on the last page into a tabular matrix that will automate smoothly. Naturally, the simplicity of this process is largely dependent on how well-structure the document you're working with is, but assuming there is some structure in place, this is a fairly straight-forward process with the help of some regular expression work. 

## Formatting (table replication)

Getting the values themselves in this tabular format is the primary goal of this post, but these will be harder to work with and debug without knowledge of the structure of the PDF document. Relying on the underlying structure of the PDF remaining the same, with a bit more regular expression work, we can replicate the table essentially as it appears in the document, grabbing the table headers and row names to populate a dataframe that mirrors the structure of the table.

Getting row names will be straight-forward enough - we can adapt the previous structure to extract the elements containing text (i.e. character strings) instead of numbers. Since they've been split at spaces, we can simply paste them back together using `str_c` from `stringr`, using a single space as the separater.

```{r formatrownames, warning=FALSE}
# Recreate row names
str_values <- map(elements, function(x) x[str_detect(x, "[A-z]")])
str_values <- map(str_values, str_c, collapse=" ")

# Drop footnotes
row_names <- flatten_chr(str_values)
row_names <- str_replace(row_names, "[0-9]", "")

# Drop header info
row_names <- row_names[(which(str_detect(row_names, "^[A-Z][a-z]")))]

row.names(g20_values) <- row_names
print(g20_values)
```

Column names are a bit trickier, considering they rely on two levels of the table. We can get the first 'row' easily enough - the "Q[1-4]" structure is unique with respect to the rest of the table, so we can seek it out and use the resulting row. We'll trim leading spaces with the `str_trim` function, and then split concatenated string into a vector where each element is a header.

```{r formatheaders, warning=FALSE}
# Recreate headers - identify quarter structure and pull
header_values <- map(elements, str_c, collapse=" ")
header_values <- flatten_chr(header_values[str_detect(header_values, "Q[1-4]")])

# Remove leading space
header_values <- str_trim(header_values, "both")

# Split to create a vector of headers
header_names <- str_split(header_values, pattern = " ", simplify = T)
##header_names <- flatten_chr(header_names)

colnames(g20_values) <- header_names
print(g20_values)
```

At this point, the only thing missing is the row of years that floats above the last eight columns. It likely makes the most sense to concatenate the years onto the column names that are lacking a year, but doing so from raw text will be highly subject to the underlying spacing structure, inviting a significant amount of error in an automated framework. Here, I'd recommend relying on the underlying structure of the releases (of a monthly newsletter, statistical release, etc.) to enforce some pre-determined structure. For example, we'd expect the same structure for every October release of the G.20. Contingent upon where we are in the year of releases, we can dynamically set a vector of years to concatenate, should we need them. I'll restrict this bit of code to deal with December only, but it can easily be built out to capture the structure of other months.

```{r formatyearheaders, warning=FALSE}
# Pull Month - Year portions of the release - the first will correspond to the month of the underlying data
months_expr <- "(January|February|March|April|May|June|July|August|September|October|November|December)"
release_no <- str_extract(g20_text[[1]], paste(months_expr, "[1-2][0-9]{3}"))

# Skeleton of control flow structure necessary to process releases - built out for October only
if(grepl("October", release_no)) {
  year_vec <- c("", "", "", "", "", "_2017", "_2017", "_2018", "_2018", "_2018", "_2018", "_2018", "_2018")
}

# Concatenate
colnames(g20_values) <- paste0(colnames(g20_values), year_vec)
print(g20_values)
```

With that, not only have the table values been converted to a tidy, tabular format, but so have the headers and row.names. With a bit more time devoted to building out the control flow necessary to process each of the 12 statistical releases for the year, this process could be automated to populated a databases of these values, which could in turn be used for dynamic analysis and visualization. With just a bit of code, functions from the `pdftools` and `stringr` libraries, and some underlying structure, PDF tables can become a part of automated processes.